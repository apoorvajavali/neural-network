{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activation functions: ['sigmoid', 'tanh', 'relu']\n",
      "Enter an activation function: tanh\n",
      "Enter number of iterations: 1000\n",
      "Enter learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 1000 iterations, the total error is 104.9999999999981\n",
      "The final weight vectors are (starting from input to output layers)\n",
      "\n",
      "W(1) - Input Layer to Hidden Layer 1:\n",
      " [[-1.92600926 -1.83010121  0.0327132   0.70120231 -1.41777193]\n",
      " [-2.15218197 -2.14520711  0.39539027  0.52936046 -0.38770273]\n",
      " [-0.37826368 -0.3018673  -0.31476798  1.10208434 -1.17876844]\n",
      " [ 0.38113581 -0.26182645  0.14918227 -0.65553504 -0.64497638]]\n",
      "\n",
      "W(2) - Hidden Layer 1 to Hidden Layer 2:\n",
      " [[ 2.99645149  2.53920411 -2.6358318 ]\n",
      " [ 2.36061332  0.57339112 -1.86062674]\n",
      " [ 0.27206588  1.36817728 -0.5084747 ]\n",
      " [-0.63411019  0.07734549  2.01258993]\n",
      " [ 3.05104758  1.83830169 -1.4243466 ]]\n",
      "\n",
      "W(3) - Hidden Layer 2 to Output Layer:\n",
      " [[-7.058561   -5.08338326 -8.15244615]\n",
      " [-4.09021504 -3.38620166 -9.20373907]\n",
      " [ 5.13038543  7.88660118  1.97207961]]\n",
      "\n",
      "Test Error: \n",
      "44.99999999999921\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import impute\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, train, h1 = 5, h2 = 3):\n",
    "        np.random.seed(1)\n",
    "        # train refers to the training dataset\n",
    "        # test refers to the testing dataset\n",
    "        # h1 and h2 represent the number of nodes in 1st and 2nd hidden layers\n",
    "\n",
    "        raw_input = train #pd.read_csv(train, header = None)\n",
    "        \n",
    "        # TODO: Remember to implement the preprocess method\n",
    "        self.X, self.y = self.preprocess(raw_input)\n",
    "        self.error = []\n",
    "        #\n",
    "        # Find number of input and output layers from the dataset\n",
    "        #\n",
    "        input_layer_size = len(self.X[0])\n",
    "        if not isinstance(self.y[0], np.ndarray):\n",
    "            output_layer_size = 1\n",
    "        else:\n",
    "            output_layer_size = len(self.y[0])\n",
    "\n",
    "        # assign random weights to matrices in network\n",
    "        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
    "        self.w01 = 2 * np.random.random((input_layer_size, h1)) - 1\n",
    "        self.X01 = self.X\n",
    "        self.delta01 = np.zeros((input_layer_size, h1))\n",
    "        self.w12 = 2 * np.random.random((h1, h2)) - 1\n",
    "        self.X12 = np.zeros((len(self.X), h1))\n",
    "        self.delta12 = np.zeros((h1, h2))\n",
    "        self.w23 = 2 * np.random.random((h2, output_layer_size)) - 1\n",
    "        self.X23 = np.zeros((len(self.X), h2))\n",
    "        self.delta23 = np.zeros((h2, output_layer_size))\n",
    "        self.deltaOut = np.zeros((output_layer_size, 1))\n",
    "    \n",
    "    #\n",
    "    # TODO: I have coded the sigmoid activation function, you need to do the same for tanh and ReLu\n",
    "    #\n",
    "\n",
    "    def __activation(self, x, activation): \n",
    "        if activation == \"sigmoid\":\n",
    "            return self.__sigmoid(x)\n",
    "        # tanh activation\n",
    "        if activation == \"tanh\":\n",
    "            return self.__tanh(x)\n",
    "        # ReLu activation\n",
    "        if activation == \"relu\":\n",
    "            return self.__relu(x)\n",
    "\n",
    "    #\n",
    "    # TODO: Define the function for tanh, ReLu and their derivatives\n",
    "    #\n",
    "\n",
    "    def __activation_derivative(self, x, activation): \n",
    "        if activation == \"sigmoid\":\n",
    "            return self.__sigmoid_derivative(x)\n",
    "        if activation == \"tanh\":\n",
    "            return self.__tanh_derivative(x)\n",
    "        if activation == \"relu\":\n",
    "            return self.__relu_derivative(x)\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # activation function tanh\n",
    "    def __tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    \n",
    "    # activation function relu\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # derivative of sigmoid function, indicates confidence about existing weight\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __tanh_derivative(self, x):\n",
    "        return 1 - (x * x)\n",
    "    \n",
    "    def __relu_derivative(self, x):\n",
    "        x[x >= 0] = 1\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "\n",
    "    #\n",
    "    # TODO: Write code for pre-processing the dataset, which would include standardization, normalization,\n",
    "    #   categorical to numerical, etc\n",
    "    #\n",
    "\n",
    "    def preprocess(self, D):\n",
    "        ncols = len(D.columns)\n",
    "        nrows = len(D.index)\n",
    "        \n",
    "        X = D.iloc[:, 0:(ncols -1)].values.reshape(nrows, ncols-1)\n",
    "        y = D.iloc[:, (ncols-1):].values.reshape(nrows, 1)\n",
    "        \n",
    "        num_imputer = impute.SimpleImputer(strategy='mean') \n",
    "        X[:, :] = num_imputer.fit_transform(X[:, :])\n",
    "        X = preprocessing.normalize(X)\n",
    "    \n",
    "        str_imputer = impute.SimpleImputer(strategy='constant') \n",
    "        y[:, :] = str_imputer.fit_transform(y[:, :])\n",
    "        \n",
    "        one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "        y = one_hot_encoder.fit_transform(y).toarray()\n",
    "\n",
    "        return (X, y)\n",
    "\n",
    "    # Below is the training function\n",
    "\n",
    "    def train(self, activation, max_iterations = 1000, learning_rate = 0.0005):\n",
    "        for iteration in range(max_iterations):\n",
    "            out = self.forward_pass(activation)\n",
    "            error = 0.5 * np.power((out - self.y), 2)\n",
    "            self.error.append(np.sum(error))\n",
    "            self.backward_pass(out, activation)\n",
    "            update_layer2 = learning_rate * self.X23.T.dot(self.deltaOut)\n",
    "            update_layer1 = learning_rate * self.X12.T.dot(self.delta23)\n",
    "            update_input = learning_rate * self.X01.T.dot(self.delta12)\n",
    "            \n",
    "            self.w23 += update_layer2\n",
    "            self.w12 += update_layer1\n",
    "            self.w01 += update_input\n",
    "            \n",
    "        plt.plot([v+1 for v in range(max_iterations)], self.error) \n",
    "        # naming the x axis \n",
    "        plt.xlabel('No of iterations') \n",
    "        # naming the y axis \n",
    "        plt.ylabel('Squared error') \n",
    "        # title to graph \n",
    "        plt.title(activation + \" activation \") \n",
    "        # function to show the plot \n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nAfter \" + str(max_iterations) + \" iterations, the total error is \" + str(np.sum(error)))\n",
    "        print(\"The final weight vectors are (starting from input to output layers)\")\n",
    "        print(\"\\nW(1) - Input Layer to Hidden Layer 1:\\n \" + str(self.w01))\n",
    "        print(\"\\nW(2) - Hidden Layer 1 to Hidden Layer 2:\\n \" + str(self.w12))\n",
    "        print(\"\\nW(3) - Hidden Layer 2 to Output Layer:\\n \" + str(self.w23))\n",
    "        \n",
    "\n",
    "    def forward_pass(self, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        in1 = np.dot(self.X, self.w01 )\n",
    "        self.X12 = self.__activation(in1, activation)   \n",
    "        in2 = np.dot(self.X12, self.w12)\n",
    "        self.X23 = self.__activation(in2, activation)   \n",
    "        in3 = np.dot(self.X23, self.w23)\n",
    "        out = self.__activation(in3, activation)        \n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, out, activation):\n",
    "        # pass our inputs through our neural network\n",
    "        self.compute_output_delta(out, activation)\n",
    "        self.compute_hidden_layer2_delta(activation)\n",
    "        self.compute_hidden_layer1_delta(activation)\n",
    "\n",
    "    def compute_output_delta(self, out, activation):\n",
    "        self.deltaOut = (self.y - out) * (self.__activation_derivative(out, activation))\n",
    "\n",
    "    def compute_hidden_layer2_delta(self, activation): \n",
    "        self.delta23 = (self.deltaOut.dot(self.w23.T)) * (self.__activation_derivative(self.X23, activation))\n",
    "\n",
    "    def compute_hidden_layer1_delta(self, activation): \n",
    "        self.delta12 = (self.delta23.dot(self.w12.T)) * (self.__activation_derivative(self.X12, activation))\n",
    "\n",
    "\n",
    "    # TODO: Implement the predict function for applying the trained model on the  test dataset.\n",
    "    # You can assume that the test dataset has the same format as the training dataset\n",
    "    # You have to output the test error from this function\n",
    "\n",
    "    def predict(self, test, activation):\n",
    "        raw_test_input = test #pd.read_csv(test, header = None)\n",
    "        X, y = self.preprocess(raw_test_input)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        out = self.forward_pass(activation)\n",
    "#         print(\"\\nTest Output:\\n\" + str(out))\n",
    "        error = 0.5 * np.power((out - self.y), 2)\n",
    "        return error\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        a_functions = ['sigmoid', 'tanh', 'relu']\n",
    "        print(\"\\nActivation functions: \" + str(a_functions))\n",
    "        activation = input(\"Enter an activation function: \")\n",
    "        if activation not in a_functions:\n",
    "            print(\"Please enter a function from the list: \" + str(a_functions))\n",
    "            activation = input(\"Enter an activation function: \")\n",
    "        iteration = input(\"Enter number of iterations: \")\n",
    "        l_rate = input(\"Enter learning rate: \")\n",
    "        \n",
    "        raw_input = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header = None)\n",
    "        raw_input = raw_input.sample(frac = 1)\n",
    "        train_input = raw_input[:105]\n",
    "        test_input = raw_input[105:]\n",
    "        \n",
    "        train_input, test_input = train_test_split(raw_input, test_size=0.3, random_state=42)\n",
    "        \n",
    "        neural_network = NeuralNet(train_input)\n",
    "        neural_network.train(activation, int(iteration), float(l_rate))\n",
    "        testError = neural_network.predict(test_input, activation)\n",
    "\n",
    "        print(\"\\nTest Error: \")\n",
    "        print(str(np.sum(testError)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
